{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will give you a short demonstration of an OSEM reconstruction with multiple motion states. \\\n",
    "\\\n",
    "There are a number of papers out there that describe this, but I'll try and give you a little bit of the maths here as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sirf.STIR as stir\n",
    "import sirf.Reg as reg\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "# add the source directory to the path\n",
    "# This is to make everything nice and self-contained\n",
    "dir_path = os.path.dirname(os.getcwd())\n",
    "source_path = os.path.join(dir_path, 'source')\n",
    "sys.path.append(source_path)\n",
    "\n",
    "from reconstruction.reconstruction import *\n",
    "from reconstruction.registration import *\n",
    "from reconstruction.osem import *\n",
    "from plotting_functions import plot_2d_image\n",
    "\n",
    "# and some additional functions for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "import imageio\n",
    "from plotting_functions import *\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we'll create some sample data. In a more clinical setting, we wouldn't need to do this. Instead, we'd start with some PET data in the form of a sinogram and some reconstructed CT images in various motion states. These would then be converted to PET attenuation units (pobably using something called a bilinear model, but don't worry about that) \\\n",
    "\\\n",
    "We'll insted use an emission image phantom (a ground truth), an attenuation image phantom and a template sinogram containing our scanner geometry. We'll use these to create attenuation images and simulated data in various motion states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's get some template data\n",
    "data_path = os.path.join(dir_path, 'data', 'template_data')\n",
    "emission_image = stir.ImageData(os.path.join(data_path, 'emission.hv'))\n",
    "attenuation_image = stir.ImageData(os.path.join(data_path, 'attenuation.hv'))\n",
    "template_sinogram = stir.AcquisitionData(os.path.join(data_path, 'template_sinogram.hs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emission_image/=100 # I'm not sure why this is necessary, but it is to get the attenuation image in reasonable units\n",
    "plt.imshow(emission_image.as_array()[0, :, :])\n",
    "plt.colorbar()\n",
    "# add point on image at 80, 80\n",
    "plt.scatter(80, 80, c='r')\n",
    "# print the value at 80, 80\n",
    "print(emission_image.as_array()[0, 80, 80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's say that we have 10 different motion state bins\n",
    "num_motion_states = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll use some helper functions to generate the motion states\n",
    "motion_emissions, motion_attenuations = [transformed_images for transformed_images in zip(*[generate_transformed_image(emission_image, attenuation_image) for i in range(num_motion_states)])]\n",
    "\n",
    "# and then use Nifty Reg to estimate the transformations\n",
    "# It would be really good if we could do this step using a neural network - Have a look at something called Voxelmorph fgor inspiration\n",
    "motion_transforms = [find_transformation_matrix(transformed_image, attenuation_image) for transformed_image in motion_attenuations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check that these transformation matrixes are sensible\n",
    "\n",
    "untransformed_images = [] # crap name, I know. These are the images that have been transformed back to the reference frame using the inverse of the transformation matrices we just estimated\n",
    "for i in range(num_motion_states):\n",
    "    resampler = reg.NiftyResample()\n",
    "    resampler.set_reference_image(emission_image)\n",
    "    resampler.set_floating_image(motion_emissions[i])\n",
    "    resampler.set_interpolation_type_to_linear()\n",
    "    resampler.add_transformation(motion_transforms[i].get_inverse())\n",
    "    resampler.process()\n",
    "    untransformed_images.append(resampler.get_output())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK and let's plot the original, transformed and \"untransformed\" images to see if they look sensible\n",
    "fig, ax  = plt.subplots(3, num_motion_states, figsize=(20, 7))\n",
    "for i in range(num_motion_states):\n",
    "    ax[0, i].imshow(emission_image.as_array()[0, :, :])\n",
    "    ax[0, i].set_title('Original')\n",
    "    ax[1, i].imshow(motion_emissions[i].as_array()[0, :, :])\n",
    "    ax[1, i].set_title('Transformed')\n",
    "    ax[2, i].imshow(untransformed_images[i].as_array()[0, :, :])\n",
    "    ax[2, i].set_title('UnTransformed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Right, now let's generate some acquisition models with the attenuation images from the motion states\n",
    "acquisition_models = [get_acquisition_model(emission_image, template_sinogram, attn, num_subsets=1) for attn in motion_attenuations]\n",
    "\n",
    "# And some simulated data using these acquisition models\n",
    "acquired_data = [get_acquired_data(image, acq_model, noise_factor=0.01) for image, acq_model in zip(motion_emissions, acquisition_models)]\n",
    "\n",
    "# acnd finally some sensitivity images for reconstruction\n",
    "# This is the step that your project will aim to replace with a neural network\n",
    "# It's not slow now because we're only using 2D images, but it will be slow when we move to 3D\n",
    "sensitvity_images = [acq_model.backward(acquired_data[i].get_uniform_copy(1)) for i, acq_model in enumerate(acquisition_models)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see what the data looks like for each motion state\n",
    "fig, ax  = plt.subplots(1, num_motion_states, figsize=(20, 4))\n",
    "for i in range(num_motion_states):\n",
    "    ax[i].imshow(acquired_data[i].as_array()[0, 0])\n",
    "    ax[i].set_title('Motion State {}'.format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare the difference to the reconstructed image when we correct for motion and when we don't \\\n",
    "\\\n",
    "We'll start by setting up the reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up and inital image for reconstruction\n",
    "\n",
    "initial_image = emission_image.get_uniform_copy(1)\n",
    "cyl = stir.TruncateToCylinderProcessor()\n",
    "cyl.set_strictly_less_than_radius(True)\n",
    "cyl.apply(initial_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the purely geometrical part of the forward projection\n",
    "\n",
    "radon = stir.AcquisitionModelUsingRayTracingMatrix()\n",
    "radon.num_subsets = 1\n",
    "radon.set_up(template_sinogram, emission_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normal formulation for MLEM, a widely used algorithm for image reconstruction is\n",
    "$$ u^{(n+1)} = \\frac{1}{s}A^\\top\\frac{f}{A u^{(n)}+ \\eta} $$\n",
    "where $u^{(n)}$ is our current image estimate, $f$ is our measured data, $A$ and $s$ ($=A^\\top \\mathbf{1}$) is our sensitivity image. \\\n",
    "\\\n",
    "If we now want to extend this to multiple motion states, we can use a subset version of the MLEM algorithm, known as OSEM where each subset of data $f_i$ is the data associated with motion state $i$. This motion subset also has an associated attenuation map, $u_i$, and an associated transform from out non-motion state, $T_i$\n",
    "Our forward model can be broken down into a number of components\n",
    "$$A_i = U(\\mu_i) G T_i$$ \n",
    "where $U(\\mu_i)$ contains the effect of attenuation on detection efficiencies and G contains the geometrical (radon) transform and detector efficiencies related to gaps in between crystals and detection efficiencies of the detectors themselves, \\\n",
    "\\\n",
    "Our Algorithm then becomes\n",
    "$$ u^{(n+1)} = \\frac{1}{s_i}A_i^\\top\\frac{f}{U(\\mu_i) G T_i u^{(n)}+ \\eta} $$\n",
    "which can be rewritten as \n",
    "$$ u^{(n+1)} = \\frac{1}{T_i^\\top G^\\top U(\\mu_i)^\\top \\mathbf{1}}T_i^\\top G^\\top U(\\mu_i)^\\top\\frac{f}{U(\\mu_i) G T_i u^{(n)}+ \\eta} $$\n",
    "and can be simplified to\n",
    "$$ u^{(n+1)} = \\frac{1}{T_i^\\top G^\\top U(\\mu_i)^\\top \\mathbf{1}}T_i^\\top G^\\top \\frac{f}{G T_i u^{(n)}+ \\eta} $$\n",
    "as $U(\\mu_i)$ is a diagonal matrix \\\n",
    "\\\n",
    "So, we have our algorithm. We transform our current image estimate to the motion state, forward project it, take the ratio with the motion state data, back project and transform back to the motionless state, and then divide by the sensitivity image in the motion state.\\\n",
    "\\\n",
    "This here is why we need to calculate the sensitivity image for every motion state. All these backprojections can be computationally intensive and se we are looking for a way to speed this up - but that's for another notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# osen reconstruction with motion correction\n",
    "osem_recon = osem_reconstruction(initial_image, radon, acquired_data, num_subsets=num_motion_states, num_epochs=10, sensitivity_images=sensitvity_images, transform_matrices=motion_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# osem reconstruction without motion correction\n",
    "osem_recon_no_correction = osem_reconstruction(initial_image, radon, acquired_data, num_subsets=num_motion_states, num_epochs=10, sensitivity_images=sensitvity_images, resample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the results\n",
    "max = min([osem_recon.max(), osem_recon_no_correction.max(), emission_image.max()])\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 3.5))\n",
    "im0 = ax[0].imshow(osem_recon.as_array()[0, :, :], vmax=max)\n",
    "ax[0].set_title('Reconstruction with motion correction')\n",
    "im1 = ax[1].imshow(osem_recon_no_correction.as_array()[0, :, :], vmax=max)\n",
    "ax[1].set_title('Reconstruction without motion correction')\n",
    "im2 = ax[2].imshow(emission_image.as_array()[0, :, :], vmax=max)\n",
    "ax[2].set_title('Ground truth')\n",
    "\n",
    "plt.colorbar(im0, ax=ax[0])\n",
    "plt.colorbar(im1, ax=ax[1])\n",
    "plt.colorbar(im2, ax=ax[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
